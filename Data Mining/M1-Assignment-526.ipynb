{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "khVZ7DoGDiBn",
   "metadata": {
    "id": "khVZ7DoGDiBn"
   },
   "source": [
    "# **M.1.Introduction to data mining** - Davis Zakary\n",
    "\n",
    "## **Assign.M.1.assignment.1** - Data Mining with Covid Data\n",
    "### **Overview and Directions**\n",
    "1. Import and manipulate a .csv file  \n",
    "2. Assess your Python Programming Skills  \n",
    "3. Other assignments are more challenging; use this to assess your skills.\n",
    "4. Prepare questions for a class discussion to help source additional tools. \n",
    "5. Perform tasks without assistance from clever sources.    \n",
    "\n",
    "#### **Desired outcomes**  \n",
    "- Experience Pandas dataframes to group, aggregate, find, sort, and calculate.  \n",
    "- Perform calculations to find the best country, rank, and total items processed. \n",
    "- Note: Pandas is reviewed in Module 2 and quality resources provided below.  \n",
    "\n",
    "#### **Additional resources**  \n",
    "- [Daniel Chen](https://github.com/chendaniely/) is a **generous** Pandas master.  \n",
    "=> Purchase of his books is recommended; not a solicitation!    \n",
    "- [Chen,D.,(2022). Pandas for everyone, 2nd.Ed.](https://www.amazon.com/Pandas-Everyone-Analysis-Addison-Wesley-Analytics/dp/0137891156/ref=sr_1_1?crid=T9BF3HU24YFL&keywords=pandas+for+everyone&qid=1685205022&sprefix=pandas+for+everyone%2Caps%2C203&sr=8-1)  \n",
    "=> [groupby](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/07-groupby.ipynb) => [missing values](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/03-missing.ipynb) => [many more!](https://github.com/chendaniely/2017-10-26-python_crash_course/tree/gh-pages/notebooks)    \n",
    "\n",
    "\n",
    "### **Task.0**  \n",
    "\n",
    "#### **Dataset**\n",
    "Source information => COVID-19 variant [sequencing](https://www.cdc.gov/coronavirus/2019-ncov/variants/genomic-surveillance.html#:~:text=Scientists%20use%20a%20process%20called%20genomic%20sequencing%20to%20identify%20SARS,test%20positive%20for%20COVID%2D19) by countries.\n",
    "  \n",
    "Data fields:\n",
    "1. `location`: the country providing information.    \n",
    "2. `date`: data entry date.  \n",
    "3. `variant`: the COVID-19 variant for the entered record.  \n",
    "4. `num_sequences`: the number of sequences **processed** by country, variant, and date.   \n",
    "5. `num_sequences_total`: the number of sequences **available** by country, variant, and date.  \n",
    "6. `perc_sequences`: the percentage of the available sequences processed (*out of 100*)  \n",
    "\n",
    "`note:` each dataset row represents *one* variant by *one* country on *one* day.  \n",
    "\n",
    "**Tasks**  \n",
    "1. Locate and read dataset into a pandas.DataFrame called 'df' via:\n",
    "    1. A Kaggle API; use existing or acquire; [Kaggle.covid.dataset](https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases?select=covid-variants.csv)   \n",
    "    2. Class github URL or another .csv method like [Matthes, Ch.16](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf) filename: [data.M.1.assignment.covid.data.csv](https://raw.githubusercontent.com/cosc-526/home.page/main/data.M.1.assignment.covid.data.csv) **consider** reading a Github data URL requires a path to **raw data**    \n",
    "2. Display the DataFrame's first 5 rows.  \n",
    "3. Display descriptive stats confirming: 100,416 data records.  \n",
    "4. Round DataFrame to 1 decimal place!   \n",
    "\n",
    "**Useful links**  \n",
    "[Built-in Functions](https://docs.python.org/3/library/functions.html#built-in-functions)  \n",
    "[pandas.DataFrame documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "J6JeWBO2Cw78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T14:38:22.213839Z",
     "start_time": "2025-06-27T14:38:21.912131Z"
    },
    "id": "J6JeWBO2Cw78"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8150b3daaa53576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T16:19:01.094804600Z",
     "start_time": "2025-06-13T16:19:00.362705600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  location        date    variant  num_sequences  perc_sequences  \\\n",
      "0   Angola  2020-07-06      Alpha              0             0.0   \n",
      "1   Angola  2020-07-06  B.1.1.277              0             0.0   \n",
      "2   Angola  2020-07-06  B.1.1.302              0             0.0   \n",
      "3   Angola  2020-07-06  B.1.1.519              0             0.0   \n",
      "4   Angola  2020-07-06    B.1.160              0             0.0   \n",
      "\n",
      "   num_sequences_total  \n",
      "0                    3  \n",
      "1                    3  \n",
      "2                    3  \n",
      "3                    3  \n",
      "4                    3  \n",
      "       num_sequences  perc_sequences  num_sequences_total\n",
      "count       100416.0        100416.0             100416.0\n",
      "mean            72.2             6.2               1509.6\n",
      "std           1669.3            21.9               8445.3\n",
      "min              0.0            -0.0                  1.0\n",
      "25%              0.0             0.0                 12.0\n",
      "50%              0.0             0.0                 59.0\n",
      "75%              0.0             0.0                394.0\n",
      "max         142280.0           100.0             146170.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/cosc-526/home.page/main/data.M.1.assignment.covid.data.csv')\n",
    "print(df.head(5))\n",
    "print(df.describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4z6COkzkC2KU",
   "metadata": {
    "id": "4z6COkzkC2KU"
   },
   "source": [
    "#### **Task.0 - Expected Outcome**  \n",
    "```\n",
    "DataFrame header\n",
    "  location  date        variant  num_sequences  perc_sequences  num_sequences_total\n",
    "0   Angola  2020-07-06      Alpha              0             0.0   3\n",
    "1   Angola  2020-07-06  B.1.1.277              0             0.0   3\n",
    "2   Angola  2020-07-06  B.1.1.302              0             0.0   3\n",
    "3   Angola  2020-07-06  B.1.1.519              0             0.0   3\n",
    "4   Angola  2020-07-06    B.1.160              0             0.0   3\n",
    "\n",
    "Dataframe descriptive statistics, rounded to tenths\n",
    "       num_sequences  perc_sequences  num_sequences_total\n",
    "count       100416.0        100416.0             100416.0\n",
    "mean            72.0             6.0               1510.0\n",
    "std           1669.0            22.0               8445.0\n",
    "min              0.0            -0.0                  1.0\n",
    "25%              0.0             0.0                 12.0\n",
    "50%              0.0             0.0                 59.0\n",
    "75%              0.0             0.0                394.0\n",
    "max         142280.0           100.0             146170.0 \n",
    "```\n",
    "\n",
    "### **Task.1** - Find uncommon variants  \n",
    "The U.S. experienced the COVID-19 `Alpha`, `Delta`, `Omicron`  \n",
    "\n",
    "**Tasks**  \n",
    "1. In whatever object you like, e.g. list, dataframe, etc  \n",
    "2. Get unique variant items for category: **`US_and_other`** where variants == [US, `non_who`, `others`]  \n",
    "3. Get unique variant items for category: **`nonUS_and_other`** where variants != [US, `non_who`, `others`]  \n",
    "4. Print your chosen objects to display unique variant categories.  \n",
    "5. Show a total unique count for each, and total for dataset,\n",
    "\n",
    "**Useful links**   \n",
    "- [len()](https://docs.python.org/3/library/functions.html#len)\n",
    "- [list comprehension w Bro Code](https://www.youtube.com/watch?v=fcLDzKH_5XM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vxMduRg5Cj5u",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T16:30:10.887826600Z",
     "start_time": "2025-06-13T16:30:10.860409300Z"
    },
    "id": "vxMduRg5Cj5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US + Other: 5\n",
      "['Alpha', 'Delta', 'Omicron', 'others', 'non_who']\n",
      "\n",
      " !(US + Other): 19\n",
      "['B.1.1.277', 'B.1.1.302', 'B.1.1.519', 'B.1.160', 'B.1.177', 'B.1.221', 'B.1.258', 'B.1.367', 'B.1.620', 'Beta', 'Epsilon', 'Eta', 'Gamma', 'Iota', 'Kappa', 'Lambda', 'Mu', 'S:677H.Robin1', 'S:677P.Pelican']\n",
      "\n",
      "Total Unique Variants: 24\n"
     ]
    }
   ],
   "source": [
    "usa = ['Alpha', 'Delta', 'Omicron', 'non_who', 'others']\n",
    "variants = df['variant'].unique()\n",
    "group1 = [x for x in variants if x in usa]\n",
    "group2 = [x for x in variants if x not in usa]\n",
    "print(f'US + Other: {len(group1)}')\n",
    "print(group1)\n",
    "\n",
    "print(f'\\n !(US + Other): {len(group2)}')\n",
    "print(group2)\n",
    "\n",
    "print(f'\\nTotal Unique Variants: {len(variants)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jBOVMJ0zck9a",
   "metadata": {
    "id": "jBOVMJ0zck9a"
   },
   "source": [
    "#### **Task.1 - Expected Outcome**  \n",
    "```\n",
    "note: organization of output can vary widely!  \n",
    "\n",
    "['Alpha', 'Delta', 'Omicron', 'others', 'non_who']  \n",
    "\n",
    "total US + other =  5\n",
    "\n",
    "['B.1.1.277', 'B.1.1.302', 'B.1.1.519', 'B.1.160', 'B.1.177', 'B.1.221',  \n",
    " 'B.1.258', 'B.1.367', 'B.1.620', 'Beta', 'Epsilon', 'Eta', 'Gamma', 'Iota',  \n",
    "  'Kappa', 'Lambda', 'Mu', 'S:677H.Robin1', 'S:677P.Pelican']   \n",
    "  \n",
    "total nonUS+other =  19   \n",
    "\n",
    "total unique variants =  24 \n",
    "```\n",
    "### **Task.2** - Find the most processed variant  \n",
    "**Tasks**  \n",
    "1. Which variant of COVID-19 has the most sequences processed?  \n",
    "2. Store and print the result in a string called **`variant_most_proc`**  \n",
    "\n",
    "**Useful links**  \n",
    "[pd.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas-dataframe-groupby), [pd.DataFrame.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html#pandas-dataframe-aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "JoFZ2N-WFDH1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:34:25.118957Z",
     "start_time": "2025-06-13T17:34:25.085197500Z"
    },
    "id": "JoFZ2N-WFDH1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta\n"
     ]
    }
   ],
   "source": [
    "groups = df.groupby(by='variant').agg({'num_sequences': 'sum'}).sort_values(by='num_sequences', ascending=False)\n",
    "variant_most_proc = groups.index[0]\n",
    "print(variant_most_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uAWiinsdBi2",
   "metadata": {
    "id": "5uAWiinsdBi2"
   },
   "source": [
    "#### **Task.2 - Expected Outcome**  \n",
    "```\n",
    "Delta  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663db8ac",
   "metadata": {
    "id": "663db8ac"
   },
   "source": [
    "### **Task.3** - Find the best country at processing ALL variant sequences  \n",
    "**Tasks**  \n",
    "1. Which country did the best processing **all** categories.    \n",
    "2. Store the result in a string called **`best_proc_country`**  \n",
    "3. The outcome is a single country.  \n",
    "4. **consider** df.groupby(\"location\").aggregate({\"num_sequences\": \"sum\", \"num_sequences_total\": \"sum\"})\n",
    "\n",
    "**Useful links**  \n",
    "[youtube: aggregate with groupby and .agg or .aggregate](https://www.youtube.com/watch?v=PNzlx3CjqAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75c31b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:37:34.910583900Z",
     "start_time": "2025-06-13T17:37:34.873525100Z"
    },
    "id": "75c31b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Cyprus                  8.192090\n",
      "Hungary                 7.962679\n",
      "Egypt                   7.768414\n",
      "United Arab Emirates    7.580321\n",
      "Uruguay                 7.264174\n",
      "                          ...   \n",
      "Seychelles              4.251074\n",
      "Slovakia                4.233719\n",
      "Fiji                    4.230159\n",
      "Brunei                  4.217019\n",
      "Vietnam                 4.180517\n",
      "Name: Percent, Length: 121, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "groups = df.groupby(by='location').agg({'num_sequences': 'sum', 'num_sequences_total': 'sum'})\n",
    "groups['Percent'] = groups['num_sequences'] / groups['num_sequences_total'] * 100\n",
    "groups.sort_values(by='Percent', inplace=True, ascending=False)\n",
    "print(groups['Percent'])\n",
    "best_proc_country = groups.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y0bRJhZLdKBu",
   "metadata": {
    "id": "y0bRJhZLdKBu"
   },
   "source": [
    "#### **Task.3 - Expected Outcome**\n",
    "```\n",
    "Total percent performed by country \n",
    "\n",
    "location                percent\n",
    "Cyprus                  8.19\n",
    "Hungary                 7.96\n",
    "Egypt                   7.77\n",
    "United Arab Emirates    7.58\n",
    "Uruguay                 7.26\n",
    "                        ... \n",
    "Seychelles              4.25\n",
    "Fiji                    4.23\n",
    "Slovakia                4.23\n",
    "Brunei                  4.22\n",
    "Vietnam                 4.18\n",
    "Name: perc_sequences, Length: 121, dtype: float64 \n",
    "\n",
    "the best country is =>  Cyprus\n",
    "```\n",
    "### **Task.4a** - Find the best country at processing specific variant sequences  \n",
    "**Tasks**  \n",
    "1. Which country is best at processing sequences for Alpha, Delta, and Omicron variants?    \n",
    "2. Store and print the result in a string called **`best_proc_country_ADO`** \n",
    "3. The final output is a single country.  \n",
    "\n",
    "**Useful links** \n",
    "- ibid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcafdd2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:40:18.472091300Z",
     "start_time": "2025-06-13T17:40:18.421052Z"
    },
    "id": "fcafdd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Vietnam       33.222530\n",
      "Brunei        32.829809\n",
      "Fiji          32.825397\n",
      "Slovakia      32.727032\n",
      "Maldives      32.562620\n",
      "                ...    \n",
      "Egypt          4.519351\n",
      "Hungary        2.965235\n",
      "Madagascar     1.165803\n",
      "Cyprus         1.129944\n",
      "Uruguay        0.000000\n",
      "Name: Percent, Length: 121, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "subset = df[df['variant'].isin(['Alpha', 'Delta', 'Omicron'])]\n",
    "groups = subset.groupby(by='location').agg({'num_sequences': 'sum', 'num_sequences_total': 'sum'})\n",
    "groups['Percent'] = groups['num_sequences'] / groups['num_sequences_total'] * 100\n",
    "groups.sort_values(by='Percent', inplace=True, ascending=False)\n",
    "print(groups['Percent'])\n",
    "best_proc_country_ADO = groups.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvANdWH2kOqc",
   "metadata": {
    "id": "fvANdWH2kOqc"
   },
   "source": [
    "#### **Task.4a - Expected Outcome**  \n",
    "```\n",
    "best_proc_country_ADO = Vietnam  \n",
    "```\n",
    "### **Task.4b** - Find the United States ranking for processing Alpha, Delta, and Omicron  \n",
    "**Tasks**  \n",
    "Given the outcome in 4a\n",
    "1. Find the positional index value for the US ranking for processing sequences for Alpha, Delta, an Omicron variants.   \n",
    "2. Store and print the ranking as an integer in a **us_ranking** variable.  \n",
    "3. Ensure your ranking scale reflects a scale starting at 1.  \n",
    "4. As a refresher, Python indexing starts at 0.  \n",
    "\n",
    "**Useful links** \n",
    "- [enumerate](https://docs.python.org/3/library/functions.html#enumerate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038e81b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:42:02.094836600Z",
     "start_time": "2025-06-13T17:42:02.068855100Z"
    },
    "id": "038e81b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "us_ranking = groups.index.tolist().index('United States') + 1\n",
    "print(us_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G5SUmnTckQBM",
   "metadata": {
    "id": "G5SUmnTckQBM"
   },
   "source": [
    "#### **Task.4b - Expected Outcome**  \n",
    "```\n",
    "United States ranking = 57  \n",
    "```\n",
    "### **Task.5.<final.task> - Write instructions for a jr. data scientist assignment**\n",
    "**Task =>**  \n",
    "- Write clear and precise directions that enable your  new junior  \n",
    "- data analyst, aka \"Jr,\" to modify and fix code that you provide.  \n",
    "#### **Grading requirements=>**  \n",
    "A clear and precise explanation of specific activities for production code your boss needs but you dont have time to fix.  \n",
    "\n",
    "Data science requires clear explanations of tasks, methodology, and effective communication with peers. To help the new junior analyst complete their first assignment, provide a concise and precise description including  \n",
    "\n",
    "1. Sample Outcome\n",
    "- Deliver a comprehensive report summarizing findings and insights from data analysis. Include, as needed, desired outcome format, data objects, and visualizations.  \n",
    "\n",
    "2. Python Code Explanation\n",
    "- Use plain language to describe specific Python code to achieve the desired outcome. Refer to pandas, Python, and other library documentation to incorporate particular language.  \n",
    "\n",
    "3. Consider Deprecated Functions\n",
    "- The provided code is outdated and broken. Encourage problem-solving skills and leverage previous experience with similar tasks. Provide relevant links for reverse engineering.  \n",
    "\n",
    "**Additional personnel considerations**\n",
    "\n",
    "4. Plain Language Explanation\n",
    "- Consider the junior analyst's background in C and provide clear and unambiguous instructions.  \n",
    "\n",
    "5. Documentation Reference\n",
    "- Emphasize where to consult pandas, Python, and other library documentation to discern code mechanics and clarify concepts.  \n",
    "\n",
    "---------------\n",
    "\n",
    "Your manager's original request => [memo substrate]\n",
    "\n",
    "*\"hey! i need by lunch the processed sequences per country on any date\n",
    "because as CFO wants to crunch numbers this afternoon - thx Lambda\"*\n",
    "\n",
    "1. Determine the percentage of processed sequences for the Alpha, Delta, and Omicron variants in the US.  \n",
    "2. Store the result as a dictionary where keys are variant names and values are percentages.  \n",
    "3. Save in variable `proc_seq_us`\n",
    "\n",
    "Other implied items based on same exercise for manager last year:\n",
    "- Determine each country's total processed sequences for Omicron on December 27, 2021 or any other date entered (date updated from 2020).\n",
    "- provide country name and # processed sequences\n",
    "- bidirectional sorting\n",
    "- store outcomes in tuple like mytuple(country_name, processed_sequen, etc.) \n",
    "- variables totals like `total_omicron_2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b5d0dc1e87cd9",
   "metadata": {},
   "source": [
    "--------------------\n",
    "### Student Response\n",
    "Okay Jr, we have an urgent ad-hoc request from the CFO that you need to look into by lunch today. Here is what I need from you:\n",
    "\n",
    "**The Goal:**\n",
    "\n",
    "For any given date, we should know how many sequences of Alpha, Delta, and Omicron have been processed by that country.\n",
    "*(This is cumulative... so you will need to sum together all data for a country with `date <= target date`.)*\n",
    "My suggestion is to create a multi-index pandas table in the following form:\n",
    "\n",
    "| Country | Variant | Date       | Processed | Available |\n",
    "|---------|---------|------------|-----------|-----------|\n",
    "| USA     | Alpha   | 2020-07-01 | 0         | 0         |\n",
    "|         |         | 2020-07-05 | 10        | 15        |\n",
    "|         |         | 2020-07-13 | 280       | 370       |\n",
    "|         | Delta   | 2020-07-01 | 0         | 0         |\n",
    "|         | ...     | ...        | ...       | ...       |\n",
    "| Canada  | Alpha   | 2020-07-01 | 0         | 0         |\n",
    "| ...     | ...     | ...        | ...       | ...       |\n",
    "\n",
    "Where *country*, *variant*, and *date* are index columns. This will let us or the CFO easily find results for any country-variant combo at a given time.\n",
    "\n",
    "In the example code from last year, we summed up the whole data frame. This is great if we only want to know the results for `date <= today`. The cumulative sum is *automatic* since we are summing up the whole time dimension.\n",
    "\n",
    "But if we want to limit results to any arbitrary date along the time axis, we need to compute the cumulative sum for every point along that time axis. You should be able to accomplish this using a clever application of the [Pandas groupyby function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html).\n",
    "* First, group by the location, variant, and date. This helps organize the data within the pandas object and prepares us for the next groupby.\n",
    "* Then, you can use the `cumsum()` function to add up along the time axis. This means you will only group by location and variant.\n",
    "\n",
    "This should give you a multi-index pandas table akin to what I outlined above. Now, you can use the [.xs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html) method to access any slice of the dataframe you need. You can divide the `num_sequences` by `num_sequences_total` to get the percentages processed on any given date.\n",
    "For example:\n",
    "* United States, Delta, all dates: `.xs(['United States', 'Delta'], level=[0, 1])`\n",
    "* All countries, Alpha, specific date: `.xs(['Alpha', '2020-08-13'], level=[1, 2])`\n",
    "* United States, everything: `.xs('United States', level=0)`\n",
    "\n",
    "Our manager has some specific requirements, which boil down to data transformation tasks once you have this \"master\" table:\n",
    "* Given a date, create a dictionary where the keys are Alpha, Delta, Omicron and the values are the percentage processed in the US. You can create a reusable function for this task.\n",
    "* Visualization of processed % for each variant in the US over time. You can use matplotlib.pyplot to accomplish this. A simple line plot with each variant as a separate series would do. Look into https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "It may be best to provide our manager with the CSV file of this master table on top of the other items. You can export the pandas table to a csv using the `.to_csv` method. Please email the results to me and I will pass along to our manager and the CFO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uV6K5AkAfDET",
   "metadata": {
    "id": "uV6K5AkAfDET"
   },
   "source": [
    "#### 5a - code you found from last year's excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qUkhMZD8T40i",
   "metadata": {
    "id": "qUkhMZD8T40i"
   },
   "outputs": [],
   "source": [
    "#=> I of III - broken code last year\n",
    "import pandas as pd\n",
    "\n",
    "#Read the data file\n",
    "url = \"https://github.com/cosc-526/home.page/raw/main/data.M.1.assignment.covid.data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "### Partially working code\n",
    "# total_omicron_2021 = []\n",
    "# #5.1\n",
    "# df = df.set_index(\"location\")\n",
    "# #5.2\n",
    "# df = df.loc[df6[\"date\"] == \"2021-12-27\"]\n",
    "# #5.3\n",
    "# df = df6.loc[df6[\"variant\"] == \"Omicron\"]\n",
    "# #5.3\n",
    "# df = df6[\"num_sequences\"]\n",
    "# #5.4\n",
    "#  = list(zip(df.index, df))\n",
    "# #5.5\n",
    "# df7 = pd.DataFrame(sorted(missing, key=lambda x: x[1], reverse=True))\n",
    "# print(df7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dzXyv7kGZlBP",
   "metadata": {
    "id": "dzXyv7kGZlBP"
   },
   "source": [
    "##### **5a - Expected Outcome**\n",
    "```\n",
    "0\t                1\n",
    "0\tUnited Kingdom\t52456\n",
    "1\tUnited States\t  24681\n",
    "2\tDenmark\t        3331\n",
    "3\tGermany\t        1701\n",
    "4\tIsrael\t        1578\n",
    "...\t...\t...\n",
    "59\tVietnam\t      1\n",
    "60\tMoldova\t      0\n",
    "61\tMonaco\t      0\n",
    "62\tNepal\t        0\n",
    "63\tSouth Korea \t0\n",
    "64 rows × 2 columns\n",
    "```\n",
    "#### 5b - code you found from last year's excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf6efa9",
   "metadata": {
    "id": "6cf6efa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, 'Omicron': 1.370817855027461}\n"
     ]
    }
   ],
   "source": [
    "#=> II of III - broken code last year\n",
    "\n",
    "proc_seq_us = {}\n",
    "df2 = df.groupby([\"location\", \"variant\"]).aggregate({\n",
    "    \"num_sequences\": \"sum\",\n",
    "    \"num_sequences_total\": \"sum\",\n",
    "})\n",
    "df2[\"perc_sequences\"] = (df2[\"num_sequences\"] / df2[\"num_sequences_total\"]) * 100\n",
    "df2 = df2.loc[(\"United States\", [\"Alpha\", \"Delta\", \"Omicron\"]), :].loc[\"United States\"]\n",
    "df2 = df2[\"perc_sequences\"]\n",
    "proc_seq_us = df2.to_dict()\n",
    "print(proc_seq_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NfYB_LjbZ_zq",
   "metadata": {
    "id": "NfYB_LjbZ_zq"
   },
   "source": [
    "##### **5b - Expected Outcome**  \n",
    "\n",
    "```\n",
    "{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, 'Omicron': 1.370817855027461}\n",
    "```\n",
    "#### 5c - code you found from last year's excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ilcLFMXTW9jG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilcLFMXTW9jG",
    "outputId": "2b827e05-e959-4580-861d-518facb7c1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0      1\n",
      "0   United Kingdom  52456\n",
      "1    United States  24681\n",
      "2          Denmark   3331\n",
      "3          Germany   1701\n",
      "4           Israel   1578\n",
      "..             ...    ...\n",
      "59         Vietnam      1\n",
      "60         Moldova      0\n",
      "61          Monaco      0\n",
      "62           Nepal      0\n",
      "63     South Korea      0\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331), ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514), ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249), ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147), ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118), ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34), ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10), ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8), ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1), ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), ('Nepal', 0), ('South Korea', 0)]\n"
     ]
    }
   ],
   "source": [
    "#=> III of III - broken code last year\n",
    "\n",
    "total_omicron_2021 = []\n",
    "#5.1\n",
    "df6 = df.set_index(\"location\")\n",
    "#5.2\n",
    "df6 = df6.loc[df6[\"date\"] == \"2021-12-27\"]\n",
    "#5.3#\n",
    "df6 = df6.loc[df6[\"variant\"] == \"Omicron\"]\n",
    "#5.3\n",
    "df6 = df6[\"num_sequences\"]\n",
    "#5.4\n",
    "total_omicron_2021 = list(zip(df6.index, df6))\n",
    "#5.5\n",
    "df7 = pd.DataFrame(sorted(total_omicron_2021, key=lambda x: x[1], reverse=True))\n",
    "print(df7)\n",
    "total_omicron_2021 = sorted(total_omicron_2021, key=lambda x: x[1], reverse=True)\n",
    "print(total_omicron_2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3c3REubaSEx",
   "metadata": {
    "id": "x3c3REubaSEx"
   },
   "source": [
    "##### **5c - Expected Outcome** \n",
    "```\n",
    "                 0      1\n",
    "0   United Kingdom  52456\n",
    "1    United States  24681\n",
    "2          Denmark   3331\n",
    "3          Germany   1701\n",
    "4           Israel   1578\n",
    "..             ...    ...\n",
    "59         Vietnam      1\n",
    "60         Moldova      0\n",
    "61          Monaco      0\n",
    "62           Nepal      0\n",
    "63     South Korea      0\n",
    "[64 rows x 2 columns]\n",
    "\n",
    "#[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331),\n",
    " ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514),\n",
    "  ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), \n",
    "  ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249),\n",
    "  ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147),\n",
    "   ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118),\n",
    "    ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), \n",
    "    ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), \n",
    "    ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34),\n",
    "     ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), \n",
    "     ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), \n",
    "     ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10),\n",
    "      ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8),\n",
    "       ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), \n",
    "       ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1),\n",
    "        ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), \n",
    "        ('Nepal', 0), ('South Korea', 0)]\n",
    "```\n",
    "## M.1. Ensure Spark is Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fb0f2213d14a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T18:19:53.345385600Z",
     "start_time": "2025-06-13T18:19:53.316386500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Microsoft\\jdk-11.0.18.10-hotspot\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-17'\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2DzgneXkPhuC",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T18:20:27.871555800Z",
     "start_time": "2025-06-13T18:20:07.984577100Z"
    },
    "id": "2DzgneXkPhuC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002B359CC49D0>\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.appName(\"Omicron Sequences\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543e212e7bd8c1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T18:20:33.863264400Z",
     "start_time": "2025-06-13T18:20:33.730712400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "el9sjRdm9gjv",
    "7o91Ig5QKVif",
    "MqZm_iFTPI7f",
    "JfIew_dgmpCQ",
    "kRoOscDyqoCO",
    "wcrKuRsWXUW1",
    "913dpFR0Fp1E",
    "kjA00fVqPjEm"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
